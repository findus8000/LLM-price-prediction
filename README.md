# LLM-price-prediction
Using the bloom LLM model to generate candlestick data. Run the main.py file to try it out. If you run in to any errors or dont get the desired results, try tweaking the inputs to the "generateData" main function and in particular the "temprature" input and "modelInputSize". You can also play around with the model prameters that you will find in the "generateText" function in the generateData file. Here is the documentation for the parameters: https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task. I found in my limited testing that a temprature of 0.7-0.9 generally works best and the rest of the settings are more dependant on each other(including modelInputSize) and effectiveness may wary. Note that the selected interval must have more datapoints than "modelInputSize" + "nrCandelsToGen". Required libraries can be found in requirements.txt. If you constantly get "KeyError: 0" you need a new api key: https://huggingface.co/settings/tokens.
